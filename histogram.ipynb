{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Histogram Based Method for Distributed IMM**"
      ],
      "metadata": {
        "id": "RLla_cjOSe-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Histogram Formation"
      ],
      "metadata": {
        "id": "9J72sSKYTayA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from collections import defaultdict, namedtuple\n",
        "from pyspark.rdd import RDD\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Define a small epsilon for floating-point comparisons\n",
        "EPSILON = 1e-9\n",
        "\n",
        "# Define the Instance and Split namedtuples\n",
        "Instance = namedtuple(\"Instance\", [\"features\", \"label\", \"weight\"])\n",
        "Split = namedtuple(\"Split\", [\"feature_index\", \"threshold\", \"categories\", \"is_continuous\"])"
      ],
      "metadata": {
        "id": "dxM0lEb6r4Av"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def samples_fraction_for_find_splits(max_bins: int, num_examples: int) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the subsample fraction for finding splits based on max_bins and num_examples.\n",
        "\n",
        "    :param max_bins: Maximum number of bins used for splitting.\n",
        "    :param num_examples: Number of examples (rows) in the dataset.\n",
        "    :return: A float representing the fraction of data to use.\n",
        "    \"\"\"\n",
        "    required_samples = max(max_bins * max_bins, 10000)\n",
        "    if required_samples < num_examples:\n",
        "        return float(required_samples) / num_examples\n",
        "    else:\n",
        "        return 1.0"
      ],
      "metadata": {
        "id": "kqbZVQXpSJNv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "F2oNxb14be7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_splits_for_continuous_feature_values(\n",
        "    feature_values,\n",
        "    is_continuous: bool,\n",
        "    num_splits: int,\n",
        "    max_bins: int,\n",
        "    total_num_examples: int\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Aggregates continuous feature values and counts, then computes split thresholds.\n",
        "\n",
        "    :param feature_values: An iterable of numeric values (floats).\n",
        "    :param is_continuous: Boolean indicating if the feature is continuous.\n",
        "    :param num_splits: The maximum number of splits allowed for this feature.\n",
        "    :param max_bins: Used for binning / quantile calculation logic.\n",
        "    :param total_num_examples: Total number of examples (rows) for fraction calculation.\n",
        "    :return: A list of Split objects representing split thresholds.\n",
        "    \"\"\"\n",
        "    if not is_continuous:\n",
        "        raise ValueError(\"find_splits_for_continuous_feature_values can only be used with a continuous feature.\")\n",
        "\n",
        "    # Aggregate values into a dictionary: {feature_value -> total_count}\n",
        "    value_counts = defaultdict(int)\n",
        "    count = 0\n",
        "\n",
        "    for v in feature_values:\n",
        "        value_counts[v] += 1  # Each data point has weight = 1\n",
        "        count += 1\n",
        "\n",
        "    # Convert to a normal dict and call the second function\n",
        "    return find_splits_for_continuous_feature_weights(\n",
        "        part_value_weights=dict(value_counts),\n",
        "        count=count,\n",
        "        is_continuous=is_continuous,\n",
        "        num_splits=num_splits,\n",
        "        max_bins=max_bins,\n",
        "        total_num_examples=total_num_examples\n",
        "    )"
      ],
      "metadata": {
        "id": "EhP6Jb8SsBcI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_splits_for_continuous_feature_weights(\n",
        "    part_value_weights: dict,\n",
        "    count: int,\n",
        "    is_continuous: bool,\n",
        "    num_splits: int,\n",
        "    max_bins: int,\n",
        "    total_num_examples: float\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Computes split thresholds for a single continuous feature.\n",
        "\n",
        "    :param part_value_weights: Dict of { feature_value -> count }.\n",
        "    :param count: Total number of data points aggregated for this feature.\n",
        "    :param is_continuous: Boolean indicating if the feature is continuous.\n",
        "    :param num_splits: The maximum number of splits allowed for this feature.\n",
        "    :param max_bins: Used for binning / quantile calculation logic.\n",
        "    :param total_num_examples: Total number of examples for fraction calculation.\n",
        "    :return: A list of Split objects representing split thresholds.\n",
        "    \"\"\"\n",
        "    if not is_continuous:\n",
        "        raise ValueError(\"find_splits_for_continuous_feature_weights can only be used with a continuous feature.\")\n",
        "\n",
        "    # If no values exist, return empty.\n",
        "    if not part_value_weights:\n",
        "        return []\n",
        "\n",
        "    # Sum of counts for this feature.\n",
        "    part_num_samples = sum(part_value_weights.values())\n",
        "\n",
        "    # Compute the fraction of the data to use for splits\n",
        "    fraction = samples_fraction_for_find_splits(\n",
        "        max_bins=max_bins,\n",
        "        num_examples=count\n",
        "    )\n",
        "\n",
        "    # Weighted number of samples (since weights are counts)\n",
        "    weighted_num_samples = fraction * float(count)\n",
        "\n",
        "    # Tolerance for floating-point adjustments\n",
        "    tolerance = EPSILON * count * 100\n",
        "\n",
        "    # Add zero-value count if needed\n",
        "    # If the expected number of samples minus the actual is greater than tolerance, add a zero count\n",
        "    if weighted_num_samples - part_num_samples > tolerance:\n",
        "        part_value_weights = dict(part_value_weights)  # Make a copy to avoid mutating the original\n",
        "        additional_count = weighted_num_samples - part_num_samples\n",
        "        part_value_weights[0.0] = part_value_weights.get(0.0, 0.0) + additional_count\n",
        "\n",
        "    # Sort the values\n",
        "    sorted_pairs = sorted(part_value_weights.items(), key=lambda x: x[0])  # [(value, count), ...]\n",
        "\n",
        "    # Number of possible splits is number of intervals between sorted values\n",
        "    possible_splits = len(sorted_pairs) - 1\n",
        "\n",
        "    if possible_splits == 0:\n",
        "        # All feature values are the same => no splits\n",
        "        return []\n",
        "\n",
        "    if possible_splits <= num_splits:\n",
        "        # If we have fewer or equal intervals compared to allowed splits, return all midpoints\n",
        "        splits = []\n",
        "        for i in range(1, len(sorted_pairs)):\n",
        "            left_val = sorted_pairs[i - 1][0]\n",
        "            right_val = sorted_pairs[i][0]\n",
        "            midpoint = (left_val + right_val) / 2.0\n",
        "            splits.append(Split(feature_index=-1, threshold=midpoint, categories=None, is_continuous=True))  # feature_index to be set later\n",
        "        return splits\n",
        "\n",
        "    # Otherwise, use stride-based approach\n",
        "    stride = weighted_num_samples / (num_splits + 1)\n",
        "\n",
        "    splits_builder = []\n",
        "    index = 1\n",
        "    current_count = sorted_pairs[0][1]\n",
        "    target_count = stride\n",
        "\n",
        "    while index < len(sorted_pairs):\n",
        "        previous_count = current_count\n",
        "        current_count += sorted_pairs[index][1]\n",
        "        previous_gap = abs(previous_count - target_count)\n",
        "        current_gap = abs(current_count - target_count)\n",
        "\n",
        "        if previous_gap < current_gap:\n",
        "            # Place a split threshold between previous value and current value\n",
        "            left_val = sorted_pairs[index - 1][0]\n",
        "            right_val = sorted_pairs[index][0]\n",
        "            midpoint = (left_val + right_val) / 2.0\n",
        "            splits_builder.append(Split(feature_index=-1, threshold=midpoint, categories=None, is_continuous=True))\n",
        "            target_count += stride\n",
        "\n",
        "        index += 1\n",
        "\n",
        "    return splits_builder"
      ],
      "metadata": {
        "id": "SYQrjDvwsJEj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_splits_for_categorical_feature(\n",
        "    categories: list,\n",
        "    counts: list,\n",
        "    is_unordered: bool,\n",
        "    num_splits: int\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Computes split thresholds for a single categorical feature.\n",
        "\n",
        "    :param categories: List of category names or indices.\n",
        "    :param counts: List of counts corresponding to each category.\n",
        "    :param is_unordered: Boolean indicating if the categorical feature is unordered.\n",
        "    :param num_splits: The maximum number of splits allowed for this feature.\n",
        "    :return: A list of Split objects representing split thresholds.\n",
        "    \"\"\"\n",
        "    if is_unordered:\n",
        "        # Handle unordered categorical features (multiclass with low arity)\n",
        "        # Find all possible subsets of categories (excluding empty and full set)\n",
        "        # For practicality, limit the number of subsets\n",
        "        # Here, we'll use one-vs-all splits for simplicity\n",
        "        splits = []\n",
        "        for cat in categories:\n",
        "            splits.append(Split(\n",
        "                feature_index=-1,\n",
        "                threshold=None,\n",
        "                categories={cat},\n",
        "                is_continuous=False\n",
        "            ))\n",
        "            if len(splits) >= num_splits:\n",
        "                break\n",
        "        return splits\n",
        "    else:\n",
        "        # Handle ordered categorical features\n",
        "        # Treat as sorted categories and find splits based on sorted order\n",
        "        sorted_categories_with_counts = sorted(zip(categories, counts), key=lambda x: x[0])\n",
        "        sorted_categories = [x[0] for x in sorted_categories_with_counts]\n",
        "        sorted_counts = [x[1] for x in sorted_categories_with_counts]\n",
        "\n",
        "        # Number of possible splits is number of categories -1\n",
        "        possible_splits = len(sorted_categories) - 1\n",
        "\n",
        "        if possible_splits <= num_splits:\n",
        "            # Return all possible splits by ordering\n",
        "            splits = []\n",
        "            for i in range(1, len(sorted_categories)):\n",
        "                left_cats = set(sorted_categories[:i])\n",
        "                splits.append(Split(\n",
        "                    feature_index=-1,\n",
        "                    threshold=None,\n",
        "                    categories=left_cats,\n",
        "                    is_continuous=False\n",
        "                ))\n",
        "            return splits\n",
        "        else:\n",
        "            # Use stride-based approach to distribute splits based on counts\n",
        "            splits = []\n",
        "            stride = sum(sorted_counts) / (num_splits + 1)\n",
        "            current_sum = 0\n",
        "            target = stride\n",
        "            left_cats = set()\n",
        "\n",
        "            for cat, cnt in zip(sorted_categories, sorted_counts):\n",
        "                current_sum += cnt\n",
        "                left_cats.add(cat)\n",
        "                if current_sum >= target:\n",
        "                    splits.append(Split(\n",
        "                        feature_index=-1,\n",
        "                        threshold=None,\n",
        "                        categories=set(left_cats),\n",
        "                        is_continuous=False\n",
        "                    ))\n",
        "                    target += stride\n",
        "                    if len(splits) >= num_splits:\n",
        "                        break\n",
        "\n",
        "            return splits"
      ],
      "metadata": {
        "id": "oDhzVyvTsOvd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_splits_by_sorting(\n",
        "    sampled_input_rdd: RDD,\n",
        "    num_features: int,\n",
        "    is_continuous: list,\n",
        "    is_unordered: list,\n",
        "    max_splits_per_feature: list,\n",
        "    max_bins: int,\n",
        "    total_weighted_examples: float\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Finds split thresholds for both continuous and categorical features by sorting and aggregating.\n",
        "\n",
        "    :param sampled_input_rdd: RDD of Instance (features, label, weight).\n",
        "    :param num_features: Total number of features.\n",
        "    :param is_continuous: List of booleans indicating if each feature is continuous.\n",
        "    :param is_unordered: List of booleans indicating if each categorical feature is unordered.\n",
        "    :param max_splits_per_feature: List of max splits allowed for each feature.\n",
        "    :param max_bins: Binning parameter used for fraction calculation logic.\n",
        "    :param total_weighted_examples: Total weighted number of examples in the dataset.\n",
        "    :return: A 2D list of Split objects. Outer list is indexed by feature, inner list contains splits for that feature.\n",
        "    \"\"\"\n",
        "    # 1. Identify continuous and categorical features\n",
        "    continuous_features = [i for i, cont in enumerate(is_continuous) if cont]\n",
        "    categorical_features = [i for i, cont in enumerate(is_continuous) if not cont]\n",
        "\n",
        "    # 2. Handle continuous features\n",
        "    continuous_splits = find_splits_for_continuous_features(\n",
        "        sampled_input_rdd=sampled_input_rdd,\n",
        "        continuous_features=continuous_features,\n",
        "        max_splits_per_feature=max_splits_per_feature,\n",
        "        max_bins=max_bins,\n",
        "        total_weighted_examples=total_weighted_examples\n",
        "    )\n",
        "\n",
        "    # 3. Handle categorical features\n",
        "    categorical_splits = find_splits_for_categorical_features(\n",
        "        sampled_input_rdd=sampled_input_rdd,\n",
        "        categorical_features=categorical_features,\n",
        "        is_unordered=is_unordered,\n",
        "        max_splits_per_feature=max_splits_per_feature\n",
        "    )\n",
        "\n",
        "    # 4. Combine splits for all features\n",
        "    all_splits = []\n",
        "    for fidx in range(num_features):\n",
        "        if is_continuous[fidx]:\n",
        "            all_splits.append(continuous_splits.get(fidx, []))\n",
        "        else:\n",
        "            all_splits.append(categorical_splits.get(fidx, []))\n",
        "\n",
        "    return all_splits"
      ],
      "metadata": {
        "id": "g8pd7UXgsUgB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_splits_for_continuous_features(\n",
        "    sampled_input_rdd: RDD,\n",
        "    continuous_features: list,\n",
        "    max_splits_per_feature: list,\n",
        "    max_bins: int,\n",
        "    total_weighted_examples: float\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Finds splits for continuous features.\n",
        "\n",
        "    :param sampled_input_rdd: RDD of Instance (features, label, weight).\n",
        "    :param continuous_features: List of feature indices that are continuous.\n",
        "    :param max_splits_per_feature: List of max splits allowed for each feature.\n",
        "    :param max_bins: Binning parameter used for fraction calculation logic.\n",
        "    :param total_weighted_examples: Total weighted number of examples in the dataset.\n",
        "    :return: Dictionary mapping feature index to list of Split objects.\n",
        "    \"\"\"\n",
        "    if not continuous_features:\n",
        "        return {}\n",
        "\n",
        "    # For each Instance, emit (featureIndex, featureValue)\n",
        "    feature_value_pairs = (\n",
        "        sampled_input_rdd\n",
        "        .flatMap(lambda inst: [\n",
        "            (i, inst.features[i])\n",
        "            for i in continuous_features\n",
        "        ])\n",
        "        .filter(lambda x: x[1] != 0.0)  # Optionally filter out zero values\n",
        "    )\n",
        "\n",
        "    # Aggregate counts for each feature and value\n",
        "    feature_aggregates = (\n",
        "        feature_value_pairs\n",
        "        .map(lambda x: (x[0], x[1]))  # (featureIndex, featureValue)\n",
        "        .map(lambda x: ((x[0], x[1]), 1))  # ((featureIndex, featureValue), 1)\n",
        "        .reduceByKey(lambda a, b: a + b)  # ((featureIndex, featureValue), count)\n",
        "        .map(lambda x: (x[0][0], (x[0][1], x[1])))  # (featureIndex, (featureValue, count))\n",
        "    )\n",
        "\n",
        "    # Collect as map: { featureIndex -> list of (featureValue, count) }\n",
        "    feature_value_counts = feature_aggregates.groupByKey().mapValues(list).collectAsMap()\n",
        "\n",
        "    # Now compute splits for each continuous feature\n",
        "    continuous_splits = {}\n",
        "    for fidx in continuous_features:\n",
        "        value_weight_map = {v: c for v, c in feature_value_counts.get(fidx, [])}\n",
        "        count = sum(value_weight_map.values())\n",
        "        splits = find_splits_for_continuous_feature_weights(\n",
        "            part_value_weights=value_weight_map,\n",
        "            count=count,\n",
        "            is_continuous=True,\n",
        "            num_splits=max_splits_per_feature[fidx],\n",
        "            max_bins=max_bins,\n",
        "            total_num_examples=total_weighted_examples\n",
        "        )\n",
        "        # Assign the correct feature index to each split\n",
        "        splits_with_index = [Split(feature_index=fidx, threshold=s.threshold, categories=None, is_continuous=True) for s in splits]\n",
        "        continuous_splits[fidx] = splits_with_index\n",
        "\n",
        "    return continuous_splits"
      ],
      "metadata": {
        "id": "zHsBL8WKsmdV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_splits_for_categorical_features(\n",
        "    sampled_input_rdd: RDD,\n",
        "    categorical_features: list,\n",
        "    is_unordered: list,\n",
        "    max_splits_per_feature: list\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Finds splits for categorical features.\n",
        "\n",
        "    :param sampled_input_rdd: RDD of Instance (features, label, weight).\n",
        "    :param categorical_features: List of feature indices that are categorical.\n",
        "    :param is_unordered: List of booleans indicating if each categorical feature is unordered.\n",
        "    :param max_splits_per_feature: List of max splits allowed for each feature.\n",
        "    :return: Dictionary mapping feature index to list of Split objects.\n",
        "    \"\"\"\n",
        "    if not categorical_features:\n",
        "        return {}\n",
        "\n",
        "    # For each Instance, emit (featureIndex, category)\n",
        "    feature_category_pairs = (\n",
        "        sampled_input_rdd\n",
        "        .flatMap(lambda inst: [\n",
        "            (i, inst.features[i])\n",
        "            for i in categorical_features\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # Aggregate counts for each feature and category\n",
        "    feature_aggregates = (\n",
        "        feature_category_pairs\n",
        "        .map(lambda x: (x[0], x[1]))  # (featureIndex, category)\n",
        "        .map(lambda x: ((x[0], x[1]), 1))  # ((featureIndex, category), 1)\n",
        "        .reduceByKey(lambda a, b: a + b)  # ((featureIndex, category), count)\n",
        "        .map(lambda x: (x[0][0], (x[0][1], x[1])))  # (featureIndex, (category, count))\n",
        "    )\n",
        "\n",
        "    # Collect as map: { featureIndex -> list of (category, count) }\n",
        "    feature_category_counts = feature_aggregates.groupByKey().mapValues(list).collectAsMap()\n",
        "\n",
        "    # Now compute splits for each categorical feature\n",
        "    categorical_splits = {}\n",
        "    for fidx in categorical_features:\n",
        "        category_count_pairs = feature_category_counts.get(fidx, [])\n",
        "        categories = [x[0] for x in category_count_pairs]\n",
        "        counts = [x[1] for x in category_count_pairs]\n",
        "        unordered = is_unordered[fidx]\n",
        "\n",
        "        splits = find_splits_for_categorical_feature(\n",
        "            categories=categories,\n",
        "            counts=counts,\n",
        "            is_unordered=unordered,\n",
        "            num_splits=max_splits_per_feature[fidx]\n",
        "        )\n",
        "        # Assign the correct feature index to each split\n",
        "        splits_with_index = [Split(feature_index=fidx, threshold=None, categories=s.categories, is_continuous=False) for s in splits]\n",
        "        categorical_splits[fidx] = splits_with_index\n",
        "\n",
        "    return categorical_splits\n"
      ],
      "metadata": {
        "id": "hBa7B2u6steX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_splits(\n",
        "    input_rdd: RDD,\n",
        "    num_features: int,\n",
        "    is_continuous: list,\n",
        "    is_unordered: list,\n",
        "    max_splits_per_feature: list,\n",
        "    max_bins: int,\n",
        "    total_weighted_examples: float,\n",
        "    seed: int\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Finds splits for decision tree calculation. Handles both continuous and categorical features.\n",
        "\n",
        "    :param input_rdd: RDD of Instances.\n",
        "    :param num_features: Number of total features.\n",
        "    :param is_continuous: List of booleans indicating if each feature is continuous.\n",
        "    :param is_unordered: List of booleans indicating if each categorical feature is unordered.\n",
        "                         Must align with categorical_features indices.\n",
        "    :param max_splits_per_feature: List of max splits allowed for each feature.\n",
        "    :param max_bins: Binning parameter used in the fraction calculation.\n",
        "    :param total_weighted_examples: Weighted number of examples in the entire dataset.\n",
        "    :param seed: Random seed for sampling.\n",
        "    :return: A 2D list (list of lists) of Split objects [featureIndex -> list of Splits].\n",
        "    \"\"\"\n",
        "    # Identify continuous and categorical features\n",
        "    continuous_features = [i for i, cont in enumerate(is_continuous) if cont]\n",
        "    categorical_features = [i for i, cont in enumerate(is_continuous) if not cont]\n",
        "\n",
        "    if not continuous_features and not categorical_features:\n",
        "        # No features to split on\n",
        "        return [[] for _ in range(num_features)]\n",
        "\n",
        "    # Compute fraction using 'samples_fraction_for_find_splits'\n",
        "    num_examples = input_rdd.count()  # Total number of examples\n",
        "    fraction = samples_fraction_for_find_splits(\n",
        "        max_bins=max_bins,\n",
        "        num_examples=num_examples\n",
        "    )\n",
        "\n",
        "    # Log the fraction (optional)\n",
        "    print(f\"Fraction of data used for calculating splits = {fraction}\")\n",
        "\n",
        "    # Sample the input if fraction < 1\n",
        "    if fraction < 1.0:\n",
        "        # PySpark sampling: sample(withReplacement, fraction, seed)\n",
        "        # To ensure reproducibility, combine seed with some randomness\n",
        "        rnd_seed = random.Random(seed).randint(0, 2**32 - 1)\n",
        "        sampled_input = input_rdd.sample(withReplacement=False, fraction=fraction, seed=rnd_seed)\n",
        "    else:\n",
        "        sampled_input = input_rdd\n",
        "\n",
        "    # Find splits by sorting\n",
        "    splits_2d = find_splits_by_sorting(\n",
        "        sampled_input_rdd=sampled_input,\n",
        "        num_features=num_features,\n",
        "        is_continuous=is_continuous,\n",
        "        is_unordered=is_unordered,\n",
        "        max_splits_per_feature=max_splits_per_feature,\n",
        "        max_bins=max_bins,\n",
        "        total_weighted_examples=total_weighted_examples\n",
        "    )\n",
        "\n",
        "    return splits_2d"
      ],
      "metadata": {
        "id": "D4jHAZjGs2da"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_splits_for_categorical_feature(\n",
        "    categories: list,\n",
        "    counts: list,\n",
        "    is_unordered: bool,\n",
        "    num_splits: int\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Computes split thresholds for a single categorical feature.\n",
        "\n",
        "    :param categories: List of category names or indices.\n",
        "    :param counts: List of counts corresponding to each category.\n",
        "    :param is_unordered: Boolean indicating if the categorical feature is unordered.\n",
        "    :param num_splits: The maximum number of splits allowed for this feature.\n",
        "    :return: A list of Split objects representing split thresholds.\n",
        "    \"\"\"\n",
        "    if is_unordered:\n",
        "        # Handle unordered categorical features (multiclass with low arity)\n",
        "        # Find all possible subsets of categories (excluding empty and full set)\n",
        "        # For practicality, limit the number of subsets\n",
        "        # Here, we'll use one-vs-all splits for simplicity\n",
        "        splits = []\n",
        "        for cat in categories:\n",
        "            splits.append(Split(\n",
        "                feature_index=-1,\n",
        "                threshold=None,\n",
        "                categories={cat},\n",
        "                is_continuous=False\n",
        "            ))\n",
        "            if len(splits) >= num_splits:\n",
        "                break\n",
        "        return splits\n",
        "    else:\n",
        "        # Handle ordered categorical features\n",
        "        # Treat as sorted categories and find splits based on sorted order\n",
        "        sorted_categories_with_counts = sorted(zip(categories, counts), key=lambda x: x[0])\n",
        "        sorted_categories = [x[0] for x in sorted_categories_with_counts]\n",
        "        sorted_counts = [x[1] for x in sorted_categories_with_counts]\n",
        "\n",
        "        # Number of possible splits is number of categories -1\n",
        "        possible_splits = len(sorted_categories) - 1\n",
        "\n",
        "        if possible_splits <= num_splits:\n",
        "            # Return all possible splits by ordering\n",
        "            splits = []\n",
        "            for i in range(1, len(sorted_categories)):\n",
        "                left_cats = set(sorted_categories[:i])\n",
        "                splits.append(Split(\n",
        "                    feature_index=-1,\n",
        "                    threshold=None,\n",
        "                    categories=left_cats,\n",
        "                    is_continuous=False\n",
        "                ))\n",
        "            return splits\n",
        "\n",
        "        # Otherwise, use stride-based approach to distribute splits based on counts\n",
        "        splits = []\n",
        "        stride = sum(sorted_counts) / (num_splits + 1)\n",
        "        current_sum = 0\n",
        "        target = stride\n",
        "        left_cats = set()\n",
        "\n",
        "        for cat, cnt in zip(sorted_categories, sorted_counts):\n",
        "            current_sum += cnt\n",
        "            left_cats.add(cat)\n",
        "            if current_sum >= target:\n",
        "                splits.append(Split(\n",
        "                    feature_index=-1,\n",
        "                    threshold=None,\n",
        "                    categories=set(left_cats),\n",
        "                    is_continuous=False\n",
        "                ))\n",
        "                target += stride\n",
        "                if len(splits) >= num_splits:\n",
        "                    break\n",
        "\n",
        "        return splits"
      ],
      "metadata": {
        "id": "OfXEiVUvs-JD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize Spark\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"FindSplitsExample\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "    sc = spark.sparkContext\n",
        "\n",
        "    # Example data: each Instance has (features, label, weight)\n",
        "    # Let's assume feature 0 is continuous and feature 1 is categorical (unordered)\n",
        "    data = [\n",
        "        Instance(features=[1.0, 'A'], label=0.0, weight=1.0),\n",
        "        Instance(features=[1.0, 'A'], label=1.0, weight=1.0),\n",
        "        Instance(features=[2.0, 'B'], label=1.0, weight=1.0),\n",
        "        Instance(features=[2.0, 'B'], label=0.0, weight=1.0),\n",
        "        Instance(features=[2.0, 'C'], label=1.0, weight=1.0),\n",
        "        Instance(features=[3.5, 'C'], label=1.0, weight=1.0),\n",
        "        Instance(features=[3.5, 'D'], label=0.0, weight=1.0),\n",
        "        Instance(features=[3.5, 'D'], label=1.0, weight=1.0),\n",
        "    ]\n",
        "    input_rdd = sc.parallelize(data)\n",
        "\n",
        "    # Define the parameters\n",
        "    num_features = 2\n",
        "    is_continuous = [True, False]  # Feature 0 is continuous, Feature 1 is categorical\n",
        "    is_unordered = [False, True]   # Feature 0 is not categorical, Feature 1 is unordered\n",
        "    max_splits_per_feature = [2, 2]  # Allow up to 2 splits per feature\n",
        "    max_bins = 32\n",
        "    total_weighted_examples = float(len(data))  # Assuming all weights are 1\n",
        "    seed = 42\n",
        "\n",
        "    # Get splits\n",
        "    splits = find_splits(\n",
        "        input_rdd=input_rdd,\n",
        "        num_features=num_features,\n",
        "        is_continuous=is_continuous,\n",
        "        is_unordered=is_unordered,\n",
        "        max_splits_per_feature=max_splits_per_feature,\n",
        "        max_bins=max_bins,\n",
        "        total_weighted_examples=total_weighted_examples,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    print(splits)\n",
        "    # Print the splits\n",
        "    for fidx, feature_splits in enumerate(splits):\n",
        "        if is_continuous[fidx]:\n",
        "            print(f\"Feature {fidx} (Continuous) splits:\")\n",
        "            for s in feature_splits:\n",
        "                print(f\"  Threshold = {s.threshold}\")\n",
        "        else:\n",
        "            print(f\"Feature {fidx} (Categorical) splits:\")\n",
        "            for s in feature_splits:\n",
        "                print(f\"  Categories = {s.categories}\")\n",
        "            if not feature_splits:\n",
        "                print(\"  No splits found.\")\n",
        "\n",
        "    # # Stop Spark\n",
        "    # spark.stop()\n"
      ],
      "metadata": {
        "id": "UZ-ZTwVfuDkT",
        "outputId": "d8495ff4-ea8b-4ce0-fa82-1fb9404270dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fraction of data used for calculating splits = 1.0\n",
            "[(0, 1.0), (0, 1.0), (0, 2.0), (0, 2.0), (0, 2.0)]\n",
            "[[Split(feature_index=0, threshold=1.5, categories=None, is_continuous=True), Split(feature_index=0, threshold=2.75, categories=None, is_continuous=True)], [Split(feature_index=1, threshold=None, categories={'A'}, is_continuous=False), Split(feature_index=1, threshold=None, categories={'B'}, is_continuous=False)]]\n",
            "Feature 0 (Continuous) splits:\n",
            "  Threshold = 1.5\n",
            "  Threshold = 2.75\n",
            "Feature 1 (Categorical) splits:\n",
            "  Categories = {'A'}\n",
            "  Categories = {'B'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first 5 rows of input_rdd\n",
        "for row in input_rdd.take(5):\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN99fYA23hTl",
        "outputId": "3b35fc5e-2f93-4d83-9774-0500d258115b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instance(features=[1.0, 'A'], label=0.0, weight=1.0)\n",
            "Instance(features=[1.0, 'A'], label=1.0, weight=1.0)\n",
            "Instance(features=[2.0, 'B'], label=1.0, weight=1.0)\n",
            "Instance(features=[2.0, 'B'], label=0.0, weight=1.0)\n",
            "Instance(features=[2.0, 'C'], label=1.0, weight=1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import the iris data set and create a dataset by duplicating it ten times. load it into a pyspark rdd. create another column features containing the feature vector of each row\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "\n",
        "\n",
        "# Load the iris dataset (replace with your actual path if needed)\n",
        "try:\n",
        "    iris_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Iris dataset: {e}\")\n",
        "    # Handle the error appropriately (e.g., exit, use a local file)\n",
        "    exit(1)\n",
        "\n",
        "# Rename columns\n",
        "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
        "\n",
        "# Duplicate the DataFrame ten times\n",
        "iris_duplicated = pd.concat([iris_df] * 10, ignore_index=True)\n",
        "\n",
        "# Convert pandas DataFrame to PySpark DataFrame\n",
        "spark_df = spark.createDataFrame(iris_duplicated)\n",
        "\n",
        "# Create the 'features' column\n",
        "from pyspark.sql.functions import array, col, udf\n",
        "from pyspark.sql.types import ArrayType, DoubleType, FloatType\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "\n",
        "feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "@udf(returnType=VectorUDT())\n",
        "def create_vector(sepal_length, sepal_width, petal_length, petal_width):\n",
        "    return Vectors.dense([sepal_length, sepal_width, petal_length, petal_width])\n",
        "\n",
        "spark_df = spark_df.withColumn('features', create_vector(*[col(c) for c in feature_cols]))\n",
        "\n",
        "# Show the first 5 rows\n",
        "spark_df.show(5)\n",
        "\n",
        "# Convert the PySpark DataFrame to an RDD\n",
        "iris_rdd = spark_df.rdd\n",
        "\n",
        "# Show the first 5 elements of the RDD\n",
        "iris_rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4uoRLQY2x5r",
        "outputId": "0cc0198e-e91f-497b-c428-550d8ff4f2c7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|    species|         features|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
            "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
            "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|\n",
            "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|\n",
            "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|\n",
            "+------------+-----------+------------+-----------+-----------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='Iris-setosa', features=DenseVector([5.1, 3.5, 1.4, 0.2])),\n",
              " Row(sepal_length=4.9, sepal_width=3.0, petal_length=1.4, petal_width=0.2, species='Iris-setosa', features=DenseVector([4.9, 3.0, 1.4, 0.2])),\n",
              " Row(sepal_length=4.7, sepal_width=3.2, petal_length=1.3, petal_width=0.2, species='Iris-setosa', features=DenseVector([4.7, 3.2, 1.3, 0.2])),\n",
              " Row(sepal_length=4.6, sepal_width=3.1, petal_length=1.5, petal_width=0.2, species='Iris-setosa', features=DenseVector([4.6, 3.1, 1.5, 0.2])),\n",
              " Row(sepal_length=5.0, sepal_width=3.6, petal_length=1.4, petal_width=0.2, species='Iris-setosa', features=DenseVector([5.0, 3.6, 1.4, 0.2]))]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameters\n",
        "num_features = 4\n",
        "is_continuous = [True, True, True, True]  # Feature 0 is continuous, Feature 1 is categorical\n",
        "is_unordered = [False, False, False, False]   # Feature 0 is not categorical, Feature 1 is unordered\n",
        "max_splits_per_feature = [10, 10, 10, 10]  # Allow up to 2 splits per feature\n",
        "max_bins = 32\n",
        "total_weighted_examples = float(len(data))  # Assuming all weights are 1\n",
        "seed = 42\n",
        "\n",
        "# Get splits\n",
        "splits = find_splits(\n",
        "    input_rdd=iris_rdd,\n",
        "    num_features=num_features,\n",
        "    is_continuous=is_continuous,\n",
        "    is_unordered=is_unordered,\n",
        "    max_splits_per_feature=max_splits_per_feature,\n",
        "    max_bins=max_bins,\n",
        "    total_weighted_examples=total_weighted_examples,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "print(splits)\n",
        "# Print the splits\n",
        "for fidx, feature_splits in enumerate(splits):\n",
        "    if is_continuous[fidx]:\n",
        "        print(f\"Feature {fidx} (Continuous) splits:\")\n",
        "        for s in feature_splits:\n",
        "            print(f\"  Threshold = {s.threshold}\")\n",
        "    else:\n",
        "        print(f\"Feature {fidx} (Categorical) splits:\")\n",
        "        for s in feature_splits:\n",
        "            print(f\"  Categories = {s.categories}\")\n",
        "        if not feature_splits:\n",
        "            print(\"  No splits found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub7OE36g6GaS",
        "outputId": "d720e6c5-a235-4a9b-f68e-65a97c86a713"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fraction of data used for calculating splits = 1.0\n",
            "[(0, 5.1), (1, 3.5), (2, 1.4), (3, 0.2), (0, 4.9)]\n",
            "[[Split(feature_index=0, threshold=4.85, categories=None, is_continuous=True), Split(feature_index=0, threshold=5.05, categories=None, is_continuous=True), Split(feature_index=0, threshold=5.15, categories=None, is_continuous=True), Split(feature_index=0, threshold=5.45, categories=None, is_continuous=True), Split(feature_index=0, threshold=5.65, categories=None, is_continuous=True), Split(feature_index=0, threshold=5.95, categories=None, is_continuous=True), Split(feature_index=0, threshold=6.15, categories=None, is_continuous=True), Split(feature_index=0, threshold=6.35, categories=None, is_continuous=True), Split(feature_index=0, threshold=6.65, categories=None, is_continuous=True), Split(feature_index=0, threshold=6.95, categories=None, is_continuous=True)], [Split(feature_index=1, threshold=2.45, categories=None, is_continuous=True), Split(feature_index=1, threshold=2.6500000000000004, categories=None, is_continuous=True), Split(feature_index=1, threshold=2.8499999999999996, categories=None, is_continuous=True), Split(feature_index=1, threshold=2.95, categories=None, is_continuous=True), Split(feature_index=1, threshold=3.05, categories=None, is_continuous=True), Split(feature_index=1, threshold=3.1500000000000004, categories=None, is_continuous=True), Split(feature_index=1, threshold=3.25, categories=None, is_continuous=True), Split(feature_index=1, threshold=3.3499999999999996, categories=None, is_continuous=True), Split(feature_index=1, threshold=3.45, categories=None, is_continuous=True), Split(feature_index=1, threshold=3.6500000000000004, categories=None, is_continuous=True)], [Split(feature_index=2, threshold=1.35, categories=None, is_continuous=True), Split(feature_index=2, threshold=1.45, categories=None, is_continuous=True), Split(feature_index=2, threshold=1.65, categories=None, is_continuous=True), Split(feature_index=2, threshold=3.55, categories=None, is_continuous=True), Split(feature_index=2, threshold=4.15, categories=None, is_continuous=True), Split(feature_index=2, threshold=4.45, categories=None, is_continuous=True), Split(feature_index=2, threshold=4.75, categories=None, is_continuous=True), Split(feature_index=2, threshold=5.05, categories=None, is_continuous=True), Split(feature_index=2, threshold=5.45, categories=None, is_continuous=True), Split(feature_index=2, threshold=5.85, categories=None, is_continuous=True)], [Split(feature_index=3, threshold=0.15000000000000002, categories=None, is_continuous=True), Split(feature_index=3, threshold=0.25, categories=None, is_continuous=True), Split(feature_index=3, threshold=0.35, categories=None, is_continuous=True), Split(feature_index=3, threshold=1.05, categories=None, is_continuous=True), Split(feature_index=3, threshold=1.25, categories=None, is_continuous=True), Split(feature_index=3, threshold=1.35, categories=None, is_continuous=True), Split(feature_index=3, threshold=1.55, categories=None, is_continuous=True), Split(feature_index=3, threshold=1.75, categories=None, is_continuous=True), Split(feature_index=3, threshold=1.95, categories=None, is_continuous=True), Split(feature_index=3, threshold=2.25, categories=None, is_continuous=True)]]\n",
            "Feature 0 (Continuous) splits:\n",
            "  Threshold = 4.85\n",
            "  Threshold = 5.05\n",
            "  Threshold = 5.15\n",
            "  Threshold = 5.45\n",
            "  Threshold = 5.65\n",
            "  Threshold = 5.95\n",
            "  Threshold = 6.15\n",
            "  Threshold = 6.35\n",
            "  Threshold = 6.65\n",
            "  Threshold = 6.95\n",
            "Feature 1 (Continuous) splits:\n",
            "  Threshold = 2.45\n",
            "  Threshold = 2.6500000000000004\n",
            "  Threshold = 2.8499999999999996\n",
            "  Threshold = 2.95\n",
            "  Threshold = 3.05\n",
            "  Threshold = 3.1500000000000004\n",
            "  Threshold = 3.25\n",
            "  Threshold = 3.3499999999999996\n",
            "  Threshold = 3.45\n",
            "  Threshold = 3.6500000000000004\n",
            "Feature 2 (Continuous) splits:\n",
            "  Threshold = 1.35\n",
            "  Threshold = 1.45\n",
            "  Threshold = 1.65\n",
            "  Threshold = 3.55\n",
            "  Threshold = 4.15\n",
            "  Threshold = 4.45\n",
            "  Threshold = 4.75\n",
            "  Threshold = 5.05\n",
            "  Threshold = 5.45\n",
            "  Threshold = 5.85\n",
            "Feature 3 (Continuous) splits:\n",
            "  Threshold = 0.15000000000000002\n",
            "  Threshold = 0.25\n",
            "  Threshold = 0.35\n",
            "  Threshold = 1.05\n",
            "  Threshold = 1.25\n",
            "  Threshold = 1.35\n",
            "  Threshold = 1.55\n",
            "  Threshold = 1.75\n",
            "  Threshold = 1.95\n",
            "  Threshold = 2.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1xOEyO1k2gJW"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}